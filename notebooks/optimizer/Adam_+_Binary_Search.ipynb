{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # For full reproducibility (slightly slower on GPU)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "xSllzC0UzaR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1 = betas[0]\n",
        "        self.beta2 = betas[1]\n",
        "        self.eps = eps\n",
        "        self.t = 0  # time step\n",
        "\n",
        "        # Initialize moment vectors\n",
        "        self.m = [torch.zeros_like(p.data) for p in self.params]  # 1st moment\n",
        "        self.v = [torch.zeros_like(p.data) for p in self.params]  # 2nd moment\n",
        "\n",
        "    def step(self):\n",
        "        self.t += 1  # Increase timestep\n",
        "\n",
        "        for i, param in enumerate(self.params):\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "\n",
        "            grad = param.grad.data\n",
        "\n",
        "            # Update biased moments\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad * grad)\n",
        "\n",
        "            # Compute bias-corrected moments\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            # Parameter update\n",
        "            param.data -= self.lr * m_hat / (v_hat.sqrt() + self.eps)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.params:\n",
        "            if param.grad is not None:\n",
        "                param.grad.detach_()\n",
        "                param.grad.zero_()\n"
      ],
      "metadata": {
        "id": "y85kqz_dyk-6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class HybridAdamBinarySearch:\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, alpha=0.5, log_sign_flips=False):\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1 = betas[0]\n",
        "        self.beta2 = betas[1]\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha  # blend factor (0.0 = pure Adam)\n",
        "        self.t = 0  # time step\n",
        "\n",
        "        self.log_sign_flips = log_sign_flips\n",
        "\n",
        "        # Adam moment vectors\n",
        "        self.m = [torch.zeros_like(p.data) for p in self.params]\n",
        "        self.v = [torch.zeros_like(p.data) for p in self.params]\n",
        "\n",
        "        # For sign-check logic\n",
        "        self.last_data_grads = [[None, None] for _ in self.params]\n",
        "\n",
        "        if self.log_sign_flips:\n",
        "            self.sign_flip_counts = [0 for _ in self.params]\n",
        "\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "\n",
        "        for i, param in enumerate(self.params):\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "\n",
        "            grad = param.grad.data\n",
        "            data = param.data\n",
        "\n",
        "            # Adam moment update\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad * grad)\n",
        "\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            update = self.lr * m_hat / (v_hat.sqrt() + self.eps)\n",
        "            adam_next = data - update  # candidate new value without correction\n",
        "\n",
        "            last_data, last_grad = self.last_data_grads[i]\n",
        "            if last_grad is not None:\n",
        "                sign_flip_mask = torch.sign(grad) * torch.sign(last_grad) < 0\n",
        "\n",
        "                if sign_flip_mask.any():\n",
        "                    if self.log_sign_flips:\n",
        "                        self.sign_flip_counts[i] += sign_flip_mask.sum().item()\n",
        "\n",
        "                    midpoint = (data + last_data) / 2\n",
        "                    # Blend midpoint with Adam update using alpha\n",
        "                    blended = self.alpha * midpoint + (1 - self.alpha) * adam_next\n",
        "                    data[sign_flip_mask] = blended[sign_flip_mask]\n",
        "\n",
        "                # For params without flip: normal Adam update\n",
        "                data[~sign_flip_mask] = adam_next[~sign_flip_mask]\n",
        "            else:\n",
        "                # First iteration — just do Adam update\n",
        "                data[:] = adam_next\n",
        "\n",
        "            # Save current state\n",
        "            self.last_data_grads[i] = [data.clone(), grad.clone()]\n",
        "\n",
        "        if self.log_sign_flips:\n",
        "            flip_summary = ', '.join(\n",
        "                f'p{i}: {count}' for i, count in enumerate(self.sign_flip_counts)\n",
        "            )\n",
        "            print(f'[Step {self.t}] Total Sign Flips: {flip_summary}')\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.params:\n",
        "            if p.grad is not None:\n",
        "                p.grad.detach_()\n",
        "                p.grad.zero_()\n"
      ],
      "metadata": {
        "id": "upQD1BdH4pTQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as adam\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Model architecture matching paper specifications\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 1000)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(1000, 1000)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(1000, 10)     # Output layer\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten input\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# MNIST data loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Data loaders with paper's batch size\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n"
      ],
      "metadata": {
        "id": "ZIJQpNcdH-Bv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aae0fa6-6e16-4800-a5ea-fb9b6fa06e76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 57.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.66MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.7MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.67MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRfZk4HD3r88",
        "outputId": "e86467da-db3f-44bc-a8a7-3c9dea6208d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.1957\n",
            "Epoch 2/10 - Loss: 0.0841\n",
            "Epoch 3/10 - Loss: 0.0568\n",
            "Epoch 4/10 - Loss: 0.0436\n",
            "Epoch 5/10 - Loss: 0.0342\n",
            "Epoch 6/10 - Loss: 0.0286\n",
            "Epoch 7/10 - Loss: 0.0260\n",
            "Epoch 8/10 - Loss: 0.0257\n",
            "Epoch 9/10 - Loss: 0.0199\n",
            "Epoch 10/10 - Loss: 0.0213\n",
            "Test Accuracy: 97.96%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize components\n",
        "model = MLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = HybridAdamBinarySearch(model.parameters(), alpha = 0)\n",
        "\n",
        "# Training loop\n",
        "def train(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print training progress\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Evaluation\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Run experiment\n",
        "train(epochs=10)  # Match paper's training duration\n",
        "test()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Epoch 1/10 - Loss: 0.1957\n",
        "Epoch 2/10 - Loss: 0.0841\n",
        "Epoch 3/10 - Loss: 0.0568\n",
        "Epoch 4/10 - Loss: 0.0436\n",
        "Epoch 5/10 - Loss: 0.0342\n",
        "Epoch 6/10 - Loss: 0.0286\n",
        "Epoch 7/10 - Loss: 0.0260\n",
        "Epoch 8/10 - Loss: 0.0257\n",
        "Epoch 9/10 - Loss: 0.0199\n",
        "Epoch 10/10 - Loss: 0.0213\n",
        "Test Accuracy: 97.96%\n",
        "'''"
      ],
      "metadata": {
        "id": "6kzT9ufC_C0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as adam\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Model architecture matching paper specifications\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 1000)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(1000, 1000)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(1000, 10)     # Output layer\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten input\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Initialize components\n",
        "model = MLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = HybridAdamBinarySearch(model.parameters(), alpha = 1)\n",
        "\n",
        "# Training loop\n",
        "def train(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print training progress\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Evaluation\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Run experiment\n",
        "train(epochs=10)  # Match paper's training duration\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN7-57yr3uSr",
        "outputId": "7233d55a-13c2-40fd-8fe3-aaaac0b98b64"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.1931\n",
            "Epoch 2/10 - Loss: 0.0746\n",
            "Epoch 3/10 - Loss: 0.0502\n",
            "Epoch 4/10 - Loss: 0.0361\n",
            "Epoch 5/10 - Loss: 0.0287\n",
            "Epoch 6/10 - Loss: 0.0252\n",
            "Epoch 7/10 - Loss: 0.0187\n",
            "Epoch 8/10 - Loss: 0.0212\n",
            "Epoch 9/10 - Loss: 0.0190\n",
            "Epoch 10/10 - Loss: 0.0142\n",
            "Test Accuracy: 97.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Epoch 1/10 - Loss: 0.1931\n",
        "Epoch 2/10 - Loss: 0.0746\n",
        "Epoch 3/10 - Loss: 0.0502\n",
        "Epoch 4/10 - Loss: 0.0361\n",
        "Epoch 5/10 - Loss: 0.0287\n",
        "Epoch 6/10 - Loss: 0.0252\n",
        "Epoch 7/10 - Loss: 0.0187\n",
        "Epoch 8/10 - Loss: 0.0212\n",
        "Epoch 9/10 - Loss: 0.0190\n",
        "Epoch 10/10 - Loss: 0.0142\n",
        "Test Accuracy: 97.85%\n",
        "'''"
      ],
      "metadata": {
        "id": "r58-w3qk4zE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8pdhsq_6k5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}