{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd8TF228M-mw",
        "outputId": "0c00d1cf-8bf5-420f-f3af-e37174eacaa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterTokenizer:\n",
        "    \"\"\"Simple character-level tokenizer for name generation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.char_to_idx = {}\n",
        "        self.idx_to_char = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def build_vocab(self, names):\n",
        "        \"\"\"Build vocabulary from list of names\"\"\"\n",
        "        # Get all unique characters\n",
        "        chars = set()\n",
        "        for name in names:\n",
        "            chars.update(name.lower())\n",
        "\n",
        "        # Add special tokens\n",
        "        chars = sorted(list(chars))\n",
        "        special_tokens = ['<PAD>', '<SOS>', '<EOS>']\n",
        "\n",
        "        # Create mappings\n",
        "        all_tokens = special_tokens + chars\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(all_tokens)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(all_tokens)}\n",
        "        self.vocab_size = len(all_tokens)\n",
        "\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"Characters: {chars}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to list of token indices\"\"\"\n",
        "        return [self.char_to_idx.get(char.lower(), 0) for char in text]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert list of indices back to text\"\"\"\n",
        "        return ''.join([self.idx_to_char.get(idx, '') for idx in indices])\n",
        "\n",
        "    def encode_name(self, name):\n",
        "        \"\"\"Encode name with SOS and EOS tokens\"\"\"\n",
        "        encoded = [self.char_to_idx['<SOS>']]\n",
        "        encoded.extend(self.encode(name))\n",
        "        encoded.append(self.char_to_idx['<EOS>'])\n",
        "        return encoded\n"
      ],
      "metadata": {
        "id": "gp4Oi7ERNDFR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NameDataset(Dataset):\n",
        "    \"\"\"Dataset class for name generation\"\"\"\n",
        "\n",
        "    def __init__(self, names, tokenizer, max_length=32):\n",
        "        self.names = names\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Encode all names\n",
        "        self.encoded_names = []\n",
        "        for name in names:\n",
        "            encoded = tokenizer.encode_name(name)\n",
        "            if len(encoded) <= max_length:\n",
        "                # Pad sequence\n",
        "                padded = encoded + [tokenizer.char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
        "                self.encoded_names.append(padded)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.encoded_names[idx], dtype=torch.long)\n",
        "        # Input is sequence[:-1], target is sequence[1:]\n",
        "        return sequence[:-1], sequence[1:]\n"
      ],
      "metadata": {
        "id": "eR-zDQwpNEuf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for transformer\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_length=512):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "0ZVh-1Y2NGia"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply attention\n",
        "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Concatenate heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.w_o(attention_output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "cEgT5T2mNITT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise feed-forward network\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n"
      ],
      "metadata": {
        "id": "wdHmXVruNKL2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Single transformer decoder block\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection\n",
        "        attn_output = self.attention(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        ff_output = self.feed_forward(self.norm2(x))\n",
        "        x = x + self.dropout(ff_output)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "WYtJCu5WNL5O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NameGeneratorTransformer(nn.Module):\n",
        "    \"\"\"Complete transformer model for name generation\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=6,\n",
        "                 d_ff=1024, max_length=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Embedding layers\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def create_causal_mask(self, seq_len):\n",
        "        \"\"\"Create causal mask to prevent attention to future tokens\"\"\"\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "        return mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        # Token embeddings\n",
        "        token_emb = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.dropout(self.positional_encoding(token_emb))\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = self.create_causal_mask(seq_len).to(x.device)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Final layer norm and projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "EglD-pVmNNeL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, tokenizer, epochs=50, lr=3e-4):\n",
        "    \"\"\"Train the transformer model\"\"\"\n",
        "\n",
        "    model.train()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits, loss = model(inputs, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f'Epoch {epoch+1}/{epochs} completed. Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Generate sample names every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(\"Sample generated names:\")\n",
        "            sample_names = generate_names(model, tokenizer, num_names=5)\n",
        "            for name in sample_names:\n",
        "                print(f\"  {name}\")\n",
        "            print()\n",
        "\n",
        "def generate_names(model, tokenizer, num_names=5, max_length=20, temperature=0.8):\n",
        "    \"\"\"Generate new names using the trained model\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    generated_names = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_names):\n",
        "            # Start with SOS token\n",
        "            current_sequence = [tokenizer.char_to_idx['<SOS>']]\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                # Convert to tensor\n",
        "                input_tensor = torch.tensor([current_sequence], dtype=torch.long).to(device)\n",
        "\n",
        "                # Get model predictions\n",
        "                logits, _ = model(input_tensor)\n",
        "\n",
        "                # Get logits for the last token\n",
        "                next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "                # Apply softmax to get probabilities\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "                # Sample next token\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "                # Check for EOS token\n",
        "                if next_token == tokenizer.char_to_idx['<EOS>']:\n",
        "                    break\n",
        "\n",
        "                # Add to sequence\n",
        "                current_sequence.append(next_token)\n",
        "\n",
        "            # Decode the sequence (excluding SOS token)\n",
        "            name = tokenizer.decode(current_sequence[1:])\n",
        "            name = name.replace('<EOS>', '').replace('<PAD>', '').strip()\n",
        "\n",
        "            if name:  # Only add non-empty names\n",
        "                generated_names.append(name)\n",
        "\n",
        "    model.train()  # Set back to training mode\n",
        "    return generated_names\n"
      ],
      "metadata": {
        "id": "9AwWRbBDNO7r"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/Indian-Male-Names.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if 'name' in df.columns:\n",
        "        # Extract the 'name' column and convert it to a list\n",
        "        name_list = df['name'].tolist()\n",
        "\n",
        "        # Print the first few names to verify\n",
        "        print(f\"Successfully read {len(name_list)} names.\")\n",
        "        print(\"First 10 names:\")\n",
        "        for i, name in enumerate(name_list[:10]):\n",
        "            print(f\"- {name}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: 'name' column not found in '{file_path}'. Available columns: {df.columns.tolist()}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{file_path}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "name_list = [x for x in name_list if isinstance(x, str)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zURdziDZNrP5",
        "outputId": "3545c8e3-68fd-479c-af3b-fdbfb6997703"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully read 14845 names.\n",
            "First 10 names:\n",
            "- barjraj\n",
            "- ramdin verma\n",
            "- sharat chandran\n",
            "- birender mandal\n",
            "- amit\n",
            "- kushal\n",
            "- kasid\n",
            "- shiv prakash\n",
            "- vikram singh\n",
            "- sanjay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8Vv3dh1OQ01"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Sample names dataset (replace with your own)\n",
        "    names = name_list\n",
        "\n",
        "    print(f\"Training on {len(names)} names\")\n",
        "\n",
        "    # Initialize tokenizer and build vocabulary\n",
        "    tokenizer = CharacterTokenizer()\n",
        "    tokenizer.build_vocab(names)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = NameDataset(names, tokenizer, max_length=32)\n",
        "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = NameGeneratorTransformer(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=256,\n",
        "        n_heads=8,\n",
        "        n_layers=6,\n",
        "        d_ff=1024,\n",
        "        max_length=32,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, dataloader, tokenizer, epochs=100, lr=3e-4)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'tokenizer_char_to_idx': tokenizer.char_to_idx,\n",
        "        'tokenizer_idx_to_char': tokenizer.idx_to_char,\n",
        "        'vocab_size': tokenizer.vocab_size\n",
        "    }, 'name_generator_model.pth')\n",
        "\n",
        "    print(\"Model saved successfully!\")\n",
        "\n",
        "    # Generate final examples\n",
        "    print(\"\\nFinal generated names:\")\n",
        "    final_names = generate_names(model, tokenizer, num_names=10, temperature=0.7)\n",
        "    for i, name in enumerate(final_names, 1):\n",
        "        print(f\"{i:2d}. {name}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KXbX0rQNQ_b",
        "outputId": "783df7ef-25c0-48d6-8b15-5a9ba0ba02d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 14821 names\n",
            "Vocabulary size: 90\n",
            "Characters: [' ', '&', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '@', '[', '\\\\', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ं', 'अ', 'आ', 'उ', 'ऐ', 'क', 'ख', 'ग', 'च', 'छ', 'ज', 'ण', 'त', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ो', '्', '\\u200d', '�']\n",
            "Model has 4785242 parameters\n",
            "Epoch 1/20, Batch 0, Loss: 4.6287\n",
            "Epoch 1/20, Batch 50, Loss: 0.8113\n",
            "Epoch 1/20, Batch 100, Loss: 0.7332\n",
            "Epoch 1/20, Batch 150, Loss: 0.6891\n",
            "Epoch 1/20, Batch 200, Loss: 0.6400\n",
            "Epoch 1/20, Batch 250, Loss: 0.6154\n",
            "Epoch 1/20, Batch 300, Loss: 0.6843\n",
            "Epoch 1/20, Batch 350, Loss: 0.6035\n",
            "Epoch 1/20, Batch 400, Loss: 0.6515\n",
            "Epoch 1/20, Batch 450, Loss: 0.6174\n",
            "Epoch 1/20, Batch 500, Loss: 0.6258\n",
            "Epoch 1/20, Batch 550, Loss: 0.5650\n",
            "Epoch 1/20, Batch 600, Loss: 0.5561\n",
            "Epoch 1/20, Batch 650, Loss: 0.4547\n",
            "Epoch 1/20, Batch 700, Loss: 0.5649\n",
            "Epoch 1/20, Batch 750, Loss: 0.6399\n",
            "Epoch 1/20, Batch 800, Loss: 0.6250\n",
            "Epoch 1/20, Batch 850, Loss: 0.5915\n",
            "Epoch 1/20, Batch 900, Loss: 0.4529\n",
            "Epoch 1/20 completed. Average Loss: 0.6838\n",
            "Epoch 2/20, Batch 0, Loss: 0.5933\n",
            "Epoch 2/20, Batch 50, Loss: 0.5407\n",
            "Epoch 2/20, Batch 100, Loss: 0.4894\n",
            "Epoch 2/20, Batch 150, Loss: 0.4938\n",
            "Epoch 2/20, Batch 200, Loss: 0.5224\n",
            "Epoch 2/20, Batch 250, Loss: 0.7402\n",
            "Epoch 2/20, Batch 300, Loss: 0.5100\n",
            "Epoch 2/20, Batch 350, Loss: 0.3683\n",
            "Epoch 2/20, Batch 400, Loss: 0.4612\n",
            "Epoch 2/20, Batch 450, Loss: 0.4823\n",
            "Epoch 2/20, Batch 500, Loss: 0.6108\n",
            "Epoch 2/20, Batch 550, Loss: 0.3755\n",
            "Epoch 2/20, Batch 600, Loss: 0.6246\n",
            "Epoch 2/20, Batch 650, Loss: 0.5366\n",
            "Epoch 2/20, Batch 700, Loss: 0.4775\n",
            "Epoch 2/20, Batch 750, Loss: 0.6061\n",
            "Epoch 2/20, Batch 800, Loss: 0.5542\n",
            "Epoch 2/20, Batch 850, Loss: 0.5635\n",
            "Epoch 2/20, Batch 900, Loss: 0.4296\n",
            "Epoch 2/20 completed. Average Loss: 0.5449\n",
            "Epoch 3/20, Batch 0, Loss: 0.3719\n",
            "Epoch 3/20, Batch 50, Loss: 0.5406\n",
            "Epoch 3/20, Batch 100, Loss: 0.5288\n",
            "Epoch 3/20, Batch 150, Loss: 0.5653\n",
            "Epoch 3/20, Batch 200, Loss: 0.5312\n",
            "Epoch 3/20, Batch 250, Loss: 0.5532\n",
            "Epoch 3/20, Batch 300, Loss: 0.5568\n",
            "Epoch 3/20, Batch 350, Loss: 0.5262\n",
            "Epoch 3/20, Batch 400, Loss: 0.3966\n",
            "Epoch 3/20, Batch 450, Loss: 0.4032\n",
            "Epoch 3/20, Batch 500, Loss: 0.5779\n",
            "Epoch 3/20, Batch 550, Loss: 0.5124\n",
            "Epoch 3/20, Batch 600, Loss: 0.4596\n",
            "Epoch 3/20, Batch 650, Loss: 0.4863\n",
            "Epoch 3/20, Batch 700, Loss: 0.4910\n",
            "Epoch 3/20, Batch 750, Loss: 0.5186\n",
            "Epoch 3/20, Batch 800, Loss: 0.3881\n",
            "Epoch 3/20, Batch 850, Loss: 0.5149\n",
            "Epoch 3/20, Batch 900, Loss: 0.4596\n",
            "Epoch 3/20 completed. Average Loss: 0.5087\n",
            "Epoch 4/20, Batch 0, Loss: 0.6190\n",
            "Epoch 4/20, Batch 50, Loss: 0.5936\n",
            "Epoch 4/20, Batch 100, Loss: 0.4980\n",
            "Epoch 4/20, Batch 150, Loss: 0.5598\n",
            "Epoch 4/20, Batch 200, Loss: 0.5648\n",
            "Epoch 4/20, Batch 250, Loss: 0.3835\n",
            "Epoch 4/20, Batch 300, Loss: 0.4902\n",
            "Epoch 4/20, Batch 350, Loss: 0.7177\n",
            "Epoch 4/20, Batch 400, Loss: 0.4993\n",
            "Epoch 4/20, Batch 450, Loss: 0.4736\n",
            "Epoch 4/20, Batch 500, Loss: 0.5970\n",
            "Epoch 4/20, Batch 550, Loss: 0.5112\n",
            "Epoch 4/20, Batch 600, Loss: 0.4543\n",
            "Epoch 4/20, Batch 650, Loss: 0.5706\n",
            "Epoch 4/20, Batch 700, Loss: 0.4986\n",
            "Epoch 4/20, Batch 750, Loss: 0.5839\n",
            "Epoch 4/20, Batch 800, Loss: 0.4288\n",
            "Epoch 4/20, Batch 850, Loss: 0.4826\n",
            "Epoch 4/20, Batch 900, Loss: 0.4207\n",
            "Epoch 4/20 completed. Average Loss: 0.4875\n",
            "Epoch 5/20, Batch 0, Loss: 0.5603\n",
            "Epoch 5/20, Batch 50, Loss: 0.5622\n",
            "Epoch 5/20, Batch 100, Loss: 0.4444\n",
            "Epoch 5/20, Batch 150, Loss: 0.5165\n",
            "Epoch 5/20, Batch 200, Loss: 0.4998\n",
            "Epoch 5/20, Batch 250, Loss: 0.5676\n",
            "Epoch 5/20, Batch 300, Loss: 0.5614\n",
            "Epoch 5/20, Batch 350, Loss: 0.4029\n",
            "Epoch 5/20, Batch 400, Loss: 0.4115\n",
            "Epoch 5/20, Batch 450, Loss: 0.5724\n",
            "Epoch 5/20, Batch 500, Loss: 0.4867\n",
            "Epoch 5/20, Batch 550, Loss: 0.4323\n",
            "Epoch 5/20, Batch 600, Loss: 0.4690\n",
            "Epoch 5/20, Batch 650, Loss: 0.4848\n",
            "Epoch 5/20, Batch 700, Loss: 0.4156\n",
            "Epoch 5/20, Batch 750, Loss: 0.4728\n",
            "Epoch 5/20, Batch 800, Loss: 0.4703\n",
            "Epoch 5/20, Batch 850, Loss: 0.4294\n",
            "Epoch 5/20, Batch 900, Loss: 0.5541\n",
            "Epoch 5/20 completed. Average Loss: 0.4729\n",
            "Epoch 6/20, Batch 0, Loss: 0.4868\n",
            "Epoch 6/20, Batch 50, Loss: 0.4259\n",
            "Epoch 6/20, Batch 100, Loss: 0.3905\n",
            "Epoch 6/20, Batch 150, Loss: 0.5270\n",
            "Epoch 6/20, Batch 200, Loss: 0.5492\n",
            "Epoch 6/20, Batch 250, Loss: 0.4009\n",
            "Epoch 6/20, Batch 300, Loss: 0.4412\n",
            "Epoch 6/20, Batch 350, Loss: 0.4547\n",
            "Epoch 6/20, Batch 400, Loss: 0.3547\n",
            "Epoch 6/20, Batch 450, Loss: 0.3650\n",
            "Epoch 6/20, Batch 500, Loss: 0.3962\n",
            "Epoch 6/20, Batch 550, Loss: 0.4548\n",
            "Epoch 6/20, Batch 600, Loss: 0.5867\n",
            "Epoch 6/20, Batch 650, Loss: 0.4119\n",
            "Epoch 6/20, Batch 700, Loss: 0.4527\n",
            "Epoch 6/20, Batch 750, Loss: 0.3876\n",
            "Epoch 6/20, Batch 800, Loss: 0.4119\n",
            "Epoch 6/20, Batch 850, Loss: 0.4249\n",
            "Epoch 6/20, Batch 900, Loss: 0.4948\n",
            "Epoch 6/20 completed. Average Loss: 0.4604\n",
            "Epoch 7/20, Batch 0, Loss: 0.4451\n",
            "Epoch 7/20, Batch 50, Loss: 0.6342\n",
            "Epoch 7/20, Batch 100, Loss: 0.4182\n",
            "Epoch 7/20, Batch 150, Loss: 0.4447\n",
            "Epoch 7/20, Batch 200, Loss: 0.4750\n",
            "Epoch 7/20, Batch 250, Loss: 0.4508\n",
            "Epoch 7/20, Batch 300, Loss: 0.4394\n",
            "Epoch 7/20, Batch 350, Loss: 0.4147\n",
            "Epoch 7/20, Batch 400, Loss: 0.4206\n",
            "Epoch 7/20, Batch 450, Loss: 0.4184\n",
            "Epoch 7/20, Batch 500, Loss: 0.4232\n",
            "Epoch 7/20, Batch 550, Loss: 0.4085\n",
            "Epoch 7/20, Batch 600, Loss: 0.4523\n",
            "Epoch 7/20, Batch 650, Loss: 0.5042\n",
            "Epoch 7/20, Batch 700, Loss: 0.4248\n",
            "Epoch 7/20, Batch 750, Loss: 0.5082\n",
            "Epoch 7/20, Batch 800, Loss: 0.5605\n",
            "Epoch 7/20, Batch 850, Loss: 0.4031\n",
            "Epoch 7/20, Batch 900, Loss: 0.5422\n",
            "Epoch 7/20 completed. Average Loss: 0.4496\n",
            "Epoch 8/20, Batch 0, Loss: 0.4223\n",
            "Epoch 8/20, Batch 50, Loss: 0.3899\n",
            "Epoch 8/20, Batch 100, Loss: 0.4100\n",
            "Epoch 8/20, Batch 150, Loss: 0.3652\n",
            "Epoch 8/20, Batch 200, Loss: 0.4344\n",
            "Epoch 8/20, Batch 250, Loss: 0.4181\n",
            "Epoch 8/20, Batch 300, Loss: 0.3777\n",
            "Epoch 8/20, Batch 350, Loss: 0.3318\n",
            "Epoch 8/20, Batch 400, Loss: 0.4980\n",
            "Epoch 8/20, Batch 450, Loss: 0.3960\n",
            "Epoch 8/20, Batch 500, Loss: 0.4489\n",
            "Epoch 8/20, Batch 550, Loss: 0.4428\n",
            "Epoch 8/20, Batch 600, Loss: 0.5933\n",
            "Epoch 8/20, Batch 650, Loss: 0.4894\n",
            "Epoch 8/20, Batch 700, Loss: 0.5608\n",
            "Epoch 8/20, Batch 750, Loss: 0.3903\n",
            "Epoch 8/20, Batch 800, Loss: 0.3475\n",
            "Epoch 8/20, Batch 850, Loss: 0.3997\n",
            "Epoch 8/20, Batch 900, Loss: 0.4121\n",
            "Epoch 8/20 completed. Average Loss: 0.4398\n",
            "Epoch 9/20, Batch 0, Loss: 0.3662\n",
            "Epoch 9/20, Batch 50, Loss: 0.5303\n",
            "Epoch 9/20, Batch 100, Loss: 0.3859\n",
            "Epoch 9/20, Batch 150, Loss: 0.4833\n",
            "Epoch 9/20, Batch 200, Loss: 0.4181\n",
            "Epoch 9/20, Batch 250, Loss: 0.4515\n",
            "Epoch 9/20, Batch 300, Loss: 0.3932\n",
            "Epoch 9/20, Batch 350, Loss: 0.5090\n",
            "Epoch 9/20, Batch 400, Loss: 0.4040\n",
            "Epoch 9/20, Batch 450, Loss: 0.3236\n",
            "Epoch 9/20, Batch 500, Loss: 0.4083\n",
            "Epoch 9/20, Batch 550, Loss: 0.4083\n",
            "Epoch 9/20, Batch 600, Loss: 0.5113\n",
            "Epoch 9/20, Batch 650, Loss: 0.4783\n",
            "Epoch 9/20, Batch 700, Loss: 0.4038\n",
            "Epoch 9/20, Batch 750, Loss: 0.4564\n",
            "Epoch 9/20, Batch 800, Loss: 0.4324\n",
            "Epoch 9/20, Batch 850, Loss: 0.4861\n",
            "Epoch 9/20, Batch 900, Loss: 0.4474\n",
            "Epoch 9/20 completed. Average Loss: 0.4315\n",
            "Epoch 10/20, Batch 0, Loss: 0.3821\n",
            "Epoch 10/20, Batch 50, Loss: 0.4225\n",
            "Epoch 10/20, Batch 100, Loss: 0.4097\n",
            "Epoch 10/20, Batch 150, Loss: 0.3907\n",
            "Epoch 10/20, Batch 200, Loss: 0.3926\n",
            "Epoch 10/20, Batch 250, Loss: 0.4257\n",
            "Epoch 10/20, Batch 300, Loss: 0.3787\n",
            "Epoch 10/20, Batch 350, Loss: 0.4093\n",
            "Epoch 10/20, Batch 400, Loss: 0.4237\n",
            "Epoch 10/20, Batch 450, Loss: 0.3127\n",
            "Epoch 10/20, Batch 500, Loss: 0.4452\n",
            "Epoch 10/20, Batch 550, Loss: 0.4334\n",
            "Epoch 10/20, Batch 600, Loss: 0.4028\n",
            "Epoch 10/20, Batch 650, Loss: 0.3769\n",
            "Epoch 10/20, Batch 700, Loss: 0.5365\n",
            "Epoch 10/20, Batch 750, Loss: 0.4323\n",
            "Epoch 10/20, Batch 800, Loss: 0.4169\n",
            "Epoch 10/20, Batch 850, Loss: 0.4949\n",
            "Epoch 10/20, Batch 900, Loss: 0.3353\n",
            "Epoch 10/20 completed. Average Loss: 0.4226\n",
            "Sample generated names:\n",
            "  kishan bahadur sharm\n",
            "  prem pal\n",
            "  sanjeev chauhan\n",
            "  rajesh\n",
            "  anand kumar\n",
            "\n",
            "Epoch 11/20, Batch 0, Loss: 0.4305\n",
            "Epoch 11/20, Batch 50, Loss: 0.3605\n",
            "Epoch 11/20, Batch 100, Loss: 0.5645\n",
            "Epoch 11/20, Batch 150, Loss: 0.3449\n",
            "Epoch 11/20, Batch 200, Loss: 0.3622\n",
            "Epoch 11/20, Batch 250, Loss: 0.3866\n",
            "Epoch 11/20, Batch 300, Loss: 0.3771\n",
            "Epoch 11/20, Batch 350, Loss: 0.4117\n",
            "Epoch 11/20, Batch 400, Loss: 0.4549\n",
            "Epoch 11/20, Batch 450, Loss: 0.3479\n",
            "Epoch 11/20, Batch 500, Loss: 0.3636\n",
            "Epoch 11/20, Batch 550, Loss: 0.3620\n",
            "Epoch 11/20, Batch 600, Loss: 0.4797\n",
            "Epoch 11/20, Batch 650, Loss: 0.4031\n",
            "Epoch 11/20, Batch 700, Loss: 0.3970\n",
            "Epoch 11/20, Batch 750, Loss: 0.3665\n",
            "Epoch 11/20, Batch 800, Loss: 0.3626\n",
            "Epoch 11/20, Batch 850, Loss: 0.3171\n",
            "Epoch 11/20, Batch 900, Loss: 0.4156\n",
            "Epoch 11/20 completed. Average Loss: 0.4143\n",
            "Epoch 12/20, Batch 0, Loss: 0.4482\n",
            "Epoch 12/20, Batch 50, Loss: 0.3245\n",
            "Epoch 12/20, Batch 100, Loss: 0.4028\n",
            "Epoch 12/20, Batch 150, Loss: 0.3861\n",
            "Epoch 12/20, Batch 200, Loss: 0.3605\n",
            "Epoch 12/20, Batch 250, Loss: 0.4045\n",
            "Epoch 12/20, Batch 300, Loss: 0.4263\n",
            "Epoch 12/20, Batch 350, Loss: 0.3800\n",
            "Epoch 12/20, Batch 400, Loss: 0.3221\n",
            "Epoch 12/20, Batch 450, Loss: 0.3680\n",
            "Epoch 12/20, Batch 500, Loss: 0.3397\n",
            "Epoch 12/20, Batch 550, Loss: 0.3632\n",
            "Epoch 12/20, Batch 600, Loss: 0.4139\n",
            "Epoch 12/20, Batch 650, Loss: 0.3796\n",
            "Epoch 12/20, Batch 700, Loss: 0.5105\n",
            "Epoch 12/20, Batch 750, Loss: 0.4238\n",
            "Epoch 12/20, Batch 800, Loss: 0.4228\n",
            "Epoch 12/20, Batch 850, Loss: 0.4631\n",
            "Epoch 12/20, Batch 900, Loss: 0.4965\n",
            "Epoch 12/20 completed. Average Loss: 0.4066\n",
            "Epoch 13/20, Batch 0, Loss: 0.4051\n",
            "Epoch 13/20, Batch 50, Loss: 0.4184\n",
            "Epoch 13/20, Batch 100, Loss: 0.5150\n",
            "Epoch 13/20, Batch 150, Loss: 0.3953\n",
            "Epoch 13/20, Batch 200, Loss: 0.3396\n",
            "Epoch 13/20, Batch 250, Loss: 0.3836\n",
            "Epoch 13/20, Batch 300, Loss: 0.4221\n",
            "Epoch 13/20, Batch 350, Loss: 0.3853\n",
            "Epoch 13/20, Batch 400, Loss: 0.4004\n",
            "Epoch 13/20, Batch 450, Loss: 0.5439\n",
            "Epoch 13/20, Batch 500, Loss: 0.3857\n",
            "Epoch 13/20, Batch 550, Loss: 0.3874\n",
            "Epoch 13/20, Batch 600, Loss: 0.3659\n",
            "Epoch 13/20, Batch 650, Loss: 0.3330\n",
            "Epoch 13/20, Batch 700, Loss: 0.4186\n",
            "Epoch 13/20, Batch 750, Loss: 0.3220\n",
            "Epoch 13/20, Batch 800, Loss: 0.3640\n",
            "Epoch 13/20, Batch 850, Loss: 0.3694\n",
            "Epoch 13/20, Batch 900, Loss: 0.2725\n",
            "Epoch 13/20 completed. Average Loss: 0.3986\n",
            "Epoch 14/20, Batch 0, Loss: 0.4145\n",
            "Epoch 14/20, Batch 50, Loss: 0.3424\n",
            "Epoch 14/20, Batch 100, Loss: 0.4043\n",
            "Epoch 14/20, Batch 150, Loss: 0.3519\n",
            "Epoch 14/20, Batch 200, Loss: 0.3691\n",
            "Epoch 14/20, Batch 250, Loss: 0.3618\n",
            "Epoch 14/20, Batch 300, Loss: 0.4475\n",
            "Epoch 14/20, Batch 350, Loss: 0.4116\n",
            "Epoch 14/20, Batch 400, Loss: 0.3382\n",
            "Epoch 14/20, Batch 450, Loss: 0.5052\n",
            "Epoch 14/20, Batch 500, Loss: 0.4402\n",
            "Epoch 14/20, Batch 550, Loss: 0.3807\n",
            "Epoch 14/20, Batch 600, Loss: 0.5388\n",
            "Epoch 14/20, Batch 650, Loss: 0.3633\n",
            "Epoch 14/20, Batch 700, Loss: 0.4027\n",
            "Epoch 14/20, Batch 750, Loss: 0.4300\n",
            "Epoch 14/20, Batch 800, Loss: 0.3889\n",
            "Epoch 14/20, Batch 850, Loss: 0.2956\n",
            "Epoch 14/20, Batch 900, Loss: 0.4395\n",
            "Epoch 14/20 completed. Average Loss: 0.3925\n",
            "Epoch 15/20, Batch 0, Loss: 0.4751\n",
            "Epoch 15/20, Batch 50, Loss: 0.2895\n",
            "Epoch 15/20, Batch 100, Loss: 0.3473\n",
            "Epoch 15/20, Batch 150, Loss: 0.3180\n",
            "Epoch 15/20, Batch 200, Loss: 0.4315\n",
            "Epoch 15/20, Batch 250, Loss: 0.3989\n",
            "Epoch 15/20, Batch 300, Loss: 0.3863\n",
            "Epoch 15/20, Batch 350, Loss: 0.3547\n",
            "Epoch 15/20, Batch 400, Loss: 0.3168\n",
            "Epoch 15/20, Batch 450, Loss: 0.3577\n",
            "Epoch 15/20, Batch 500, Loss: 0.3803\n",
            "Epoch 15/20, Batch 550, Loss: 0.3440\n",
            "Epoch 15/20, Batch 600, Loss: 0.3169\n",
            "Epoch 15/20, Batch 650, Loss: 0.3781\n",
            "Epoch 15/20, Batch 700, Loss: 0.3072\n",
            "Epoch 15/20, Batch 750, Loss: 0.3005\n",
            "Epoch 15/20, Batch 800, Loss: 0.4760\n",
            "Epoch 15/20, Batch 850, Loss: 0.3662\n",
            "Epoch 15/20, Batch 900, Loss: 0.3472\n",
            "Epoch 15/20 completed. Average Loss: 0.3857\n",
            "Epoch 16/20, Batch 0, Loss: 0.4624\n",
            "Epoch 16/20, Batch 50, Loss: 0.3506\n",
            "Epoch 16/20, Batch 100, Loss: 0.3039\n",
            "Epoch 16/20, Batch 150, Loss: 0.4631\n",
            "Epoch 16/20, Batch 200, Loss: 0.3864\n",
            "Epoch 16/20, Batch 250, Loss: 0.3433\n",
            "Epoch 16/20, Batch 300, Loss: 0.4132\n",
            "Epoch 16/20, Batch 350, Loss: 0.3806\n",
            "Epoch 16/20, Batch 400, Loss: 0.3103\n",
            "Epoch 16/20, Batch 450, Loss: 0.3179\n",
            "Epoch 16/20, Batch 500, Loss: 0.3339\n",
            "Epoch 16/20, Batch 550, Loss: 0.4553\n",
            "Epoch 16/20, Batch 600, Loss: 0.4201\n",
            "Epoch 16/20, Batch 650, Loss: 0.4604\n",
            "Epoch 16/20, Batch 700, Loss: 0.3739\n",
            "Epoch 16/20, Batch 750, Loss: 0.3809\n",
            "Epoch 16/20, Batch 800, Loss: 0.2852\n",
            "Epoch 16/20, Batch 850, Loss: 0.3196\n",
            "Epoch 16/20, Batch 900, Loss: 0.3419\n",
            "Epoch 16/20 completed. Average Loss: 0.3805\n",
            "Epoch 17/20, Batch 0, Loss: 0.4263\n",
            "Epoch 17/20, Batch 50, Loss: 0.3420\n",
            "Epoch 17/20, Batch 100, Loss: 0.3900\n",
            "Epoch 17/20, Batch 150, Loss: 0.2967\n",
            "Epoch 17/20, Batch 200, Loss: 0.3558\n",
            "Epoch 17/20, Batch 250, Loss: 0.3763\n",
            "Epoch 17/20, Batch 300, Loss: 0.3919\n",
            "Epoch 17/20, Batch 350, Loss: 0.4276\n",
            "Epoch 17/20, Batch 400, Loss: 0.3087\n",
            "Epoch 17/20, Batch 450, Loss: 0.3998\n",
            "Epoch 17/20, Batch 500, Loss: 0.3258\n",
            "Epoch 17/20, Batch 550, Loss: 0.4175\n",
            "Epoch 17/20, Batch 600, Loss: 0.3064\n",
            "Epoch 17/20, Batch 650, Loss: 0.4109\n",
            "Epoch 17/20, Batch 700, Loss: 0.3362\n",
            "Epoch 17/20, Batch 750, Loss: 0.3258\n",
            "Epoch 17/20, Batch 800, Loss: 0.4178\n",
            "Epoch 17/20, Batch 850, Loss: 0.4409\n",
            "Epoch 17/20, Batch 900, Loss: 0.3362\n",
            "Epoch 17/20 completed. Average Loss: 0.3763\n",
            "Epoch 18/20, Batch 0, Loss: 0.4167\n",
            "Epoch 18/20, Batch 50, Loss: 0.3135\n",
            "Epoch 18/20, Batch 100, Loss: 0.3850\n",
            "Epoch 18/20, Batch 150, Loss: 0.3524\n",
            "Epoch 18/20, Batch 200, Loss: 0.3935\n",
            "Epoch 18/20, Batch 250, Loss: 0.4237\n",
            "Epoch 18/20, Batch 300, Loss: 0.4012\n",
            "Epoch 18/20, Batch 350, Loss: 0.3203\n",
            "Epoch 18/20, Batch 400, Loss: 0.4139\n",
            "Epoch 18/20, Batch 450, Loss: 0.3414\n",
            "Epoch 18/20, Batch 500, Loss: 0.3617\n",
            "Epoch 18/20, Batch 550, Loss: 0.3878\n",
            "Epoch 18/20, Batch 600, Loss: 0.3742\n",
            "Epoch 18/20, Batch 650, Loss: 0.2825\n",
            "Epoch 18/20, Batch 700, Loss: 0.3588\n",
            "Epoch 18/20, Batch 750, Loss: 0.3504\n",
            "Epoch 18/20, Batch 800, Loss: 0.4322\n",
            "Epoch 18/20, Batch 850, Loss: 0.3978\n",
            "Epoch 18/20, Batch 900, Loss: 0.3703\n",
            "Epoch 18/20 completed. Average Loss: 0.3730\n",
            "Epoch 19/20, Batch 0, Loss: 0.3723\n",
            "Epoch 19/20, Batch 50, Loss: 0.4077\n",
            "Epoch 19/20, Batch 100, Loss: 0.3413\n",
            "Epoch 19/20, Batch 150, Loss: 0.3672\n",
            "Epoch 19/20, Batch 200, Loss: 0.2992\n",
            "Epoch 19/20, Batch 250, Loss: 0.3645\n",
            "Epoch 19/20, Batch 300, Loss: 0.3127\n",
            "Epoch 19/20, Batch 350, Loss: 0.4044\n",
            "Epoch 19/20, Batch 400, Loss: 0.3672\n",
            "Epoch 19/20, Batch 450, Loss: 0.3709\n",
            "Epoch 19/20, Batch 500, Loss: 0.5236\n",
            "Epoch 19/20, Batch 550, Loss: 0.3670\n",
            "Epoch 19/20, Batch 600, Loss: 0.3626\n",
            "Epoch 19/20, Batch 650, Loss: 0.4963\n",
            "Epoch 19/20, Batch 700, Loss: 0.3370\n",
            "Epoch 19/20, Batch 750, Loss: 0.3948\n",
            "Epoch 19/20, Batch 800, Loss: 0.3862\n",
            "Epoch 19/20, Batch 850, Loss: 0.3868\n",
            "Epoch 19/20, Batch 900, Loss: 0.2891\n",
            "Epoch 19/20 completed. Average Loss: 0.3706\n",
            "Epoch 20/20, Batch 0, Loss: 0.4264\n",
            "Epoch 20/20, Batch 50, Loss: 0.4202\n",
            "Epoch 20/20, Batch 100, Loss: 0.4120\n",
            "Epoch 20/20, Batch 150, Loss: 0.3884\n",
            "Epoch 20/20, Batch 200, Loss: 0.3624\n",
            "Epoch 20/20, Batch 250, Loss: 0.3383\n",
            "Epoch 20/20, Batch 300, Loss: 0.3282\n",
            "Epoch 20/20, Batch 350, Loss: 0.3425\n",
            "Epoch 20/20, Batch 400, Loss: 0.3758\n",
            "Epoch 20/20, Batch 450, Loss: 0.4092\n",
            "Epoch 20/20, Batch 500, Loss: 0.3130\n",
            "Epoch 20/20, Batch 550, Loss: 0.3696\n",
            "Epoch 20/20, Batch 600, Loss: 0.3597\n",
            "Epoch 20/20, Batch 650, Loss: 0.3402\n",
            "Epoch 20/20, Batch 700, Loss: 0.3801\n",
            "Epoch 20/20, Batch 750, Loss: 0.3996\n",
            "Epoch 20/20, Batch 800, Loss: 0.3506\n",
            "Epoch 20/20, Batch 850, Loss: 0.4123\n",
            "Epoch 20/20, Batch 900, Loss: 0.3708\n",
            "Epoch 20/20 completed. Average Loss: 0.3690\n",
            "Sample generated names:\n",
            "  pankaj pandit\n",
            "  ramesh chand sharma\n",
            "  jagdish\n",
            "  vijay\n",
            "  mukesh\n",
            "\n",
            "Model saved successfully!\n",
            "\n",
            "Final generated names:\n",
            " 1. mohd anwar\n",
            " 2. manoj\n",
            " 3. ramji\n",
            " 4. rajender singh\n",
            " 5. rajkumar\n",
            " 6. shankar lal arora\n",
            " 7. rajpal\n",
            " 8. chand mohammad\n",
            " 9. lakhan chand\n",
            "10. bablu ram\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_generate():\n",
        "    \"\"\"Load trained model and generate new names\"\"\"\n",
        "\n",
        "    # Load the saved model\n",
        "    checkpoint = torch.load('name_generator_model.pth', map_location=device)\n",
        "\n",
        "    # Recreate tokenizer\n",
        "    tokenizer = CharacterTokenizer()\n",
        "    tokenizer.char_to_idx = checkpoint['tokenizer_char_to_idx']\n",
        "    tokenizer.idx_to_char = checkpoint['tokenizer_idx_to_char']\n",
        "    tokenizer.vocab_size = checkpoint['vocab_size']\n",
        "\n",
        "    # Recreate model\n",
        "    model = NameGeneratorTransformer(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=256,\n",
        "        n_heads=8,\n",
        "        n_layers=6,\n",
        "        d_ff=1024,\n",
        "        max_length=32,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    # Load model weights\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Interactive generation\n",
        "    while True:\n",
        "        try:\n",
        "            num_names = int(input(\"\\nHow many names to generate? (0 to exit): \"))\n",
        "            if num_names == 0:\n",
        "                break\n",
        "\n",
        "            temperature = float(input(\"Temperature (0.1-2.0, higher = more creative): \") or \"0.8\")\n",
        "            temperature = max(0.1, min(2.0, temperature))\n",
        "\n",
        "            print(f\"\\nGenerating {num_names} names with temperature {temperature}:\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            generated_names = generate_names(model, tokenizer, num_names, temperature=temperature)\n",
        "\n",
        "            for i, name in enumerate(generated_names, 1):\n",
        "                print(f\"{i:2d}. {name.capitalize()}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "        except ValueError:\n",
        "            print(\"Please enter valid numbers.\")\n",
        "\n",
        "    print(\"Goodbye!\")\n",
        "\n",
        "def generate_with_prefix(model, tokenizer, prefix=\"\", max_length=20, temperature=0.8):\n",
        "    \"\"\"Generate names starting with a specific prefix\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Start with SOS token and prefix\n",
        "        current_sequence = [tokenizer.char_to_idx['<SOS>']]\n",
        "\n",
        "        # Add prefix characters\n",
        "        for char in prefix.lower():\n",
        "            if char in tokenizer.char_to_idx:\n",
        "                current_sequence.append(tokenizer.char_to_idx[char])\n",
        "\n",
        "        # Generate remaining characters\n",
        "        for _ in range(max_length - len(prefix)):\n",
        "            input_tensor = torch.tensor([current_sequence], dtype=torch.long).to(device)\n",
        "            logits, _ = model(input_tensor)\n",
        "\n",
        "            next_token_logits = logits[0, -1, :] / temperature\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            if next_token == tokenizer.char_to_idx['<EOS>']:\n",
        "                break\n",
        "\n",
        "            current_sequence.append(next_token)\n",
        "\n",
        "        # Decode (excluding SOS token)\n",
        "        name = tokenizer.decode(current_sequence[1:])\n",
        "        name = name.replace('<EOS>', '').replace('<PAD>', '').strip()\n",
        "\n",
        "    return name\n",
        "\n",
        "# Example usage for inference\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment to run inference\n",
        "    load_model_and_generate()\n",
        "    # pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttssYp_RNUh2",
        "outputId": "3c0d57cc-3cdb-48a7-ceb1-3d3e38e42e8b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\n",
            "How many names to generate? (0 to exit): 10\n",
            "Temperature (0.1-2.0, higher = more creative): 1.9\n",
            "\n",
            "Generating 10 names with temperature 1.9:\n",
            "----------------------------------------\n",
            " 1. Hawdik\n",
            " 2. Tarun\n",
            " 3. Nishant@mukhterju\n",
            " 4. Ganni kumwसja\n",
            " 5. Bhrop s/-v0`sha`hveय\n",
            " 6. Deshraj\n",
            " 7. Yhmed ahmad\n",
            " 8. Tepyendra\n",
            " 9. Hasa raj\n",
            "10. Chachhchuqbu\n",
            "\n",
            "How many names to generate? (0 to exit): 2\n",
            "Temperature (0.1-2.0, higher = more creative): 2\n",
            "\n",
            "Generating 2 names with temperature 2.0:\n",
            "----------------------------------------\n",
            " 1. Farha[t\n",
            " 2. Cनवदील\n",
            "\n",
            "How many names to generate? (0 to exit): 10\n",
            "Temperature (0.1-2.0, higher = more creative): 2\n",
            "\n",
            "Generating 10 names with temperature 2.0:\n",
            "----------------------------------------\n",
            " 1. Nikhi.\n",
            " 2. Noor alamhe, अलाल@p\n",
            " 3. श्री च\n",
            " 4. Raghuram prehi\n",
            " 5. Hariis\n",
            " 6. Ctvan parkeet,bepual\n",
            " 7. Schhiter\n",
            " 8. Saaoad\n",
            " 9. /o9यn\n",
            "10. Awissh magwa @ jita)\n",
            "\n",
            "How many names to generate? (0 to exit): 0\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MdRKyUGGNW83"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}